"""
分类树，三步：
1、实例化，建立评估模型对象
2、通过模型接口训练模型
3、通过模型接口提取需要的信息
"""
"""
# 该流程的代码
from sklearn import tree                    #导入训练模块

clf = tree.DecisionTreeClassifier()         #实例化
clf = clf.fit(X_train, y_train)             #用训练集数据训练模型--fit
result = clf.score(X_test, y_test)          #导入测试集，从接口中调用需要的信息
"""

"""
# DecisionTreeClassifier

class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)

重要参数：
criterion   1)输入entropy，使用信息熵   当维度不够时用
            2)输入gini，使用基尼系数    维度、噪音大用

"""

from sklearn import tree
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split

wine = load_wine()
# 获取数据
print(wine.data)
# 获取标签
print(wine.target)

# 可用于查看数据
import pandas as pd
print(pd.concat([pd.DataFrame(wine.data), pd.DataFrame(wine.target)], axis=1))

# 标签的名称
print(wine.feature_names)
# 标签的分类
print(wine.target_names)


"""
# 三个参数--标签、特征、测试样本的尺寸
# 前面的顺序不能错，必须使X X y y
# X_train, X_test是数据     y_train, y_test是标签
# train_test_split是随机划分测试集和训练集，所以score可能不同
"""
X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3)

# 标签尺寸--矩阵大小
print(X_train.shape)
print(wine.data.shape)      # X_train 是 wine.data 的70%

# 建模--统一三行代码
clf = tree.DecisionTreeClassifier(criterion="entropy")
clf = clf.fit(X_train, y_train)
score = clf.score(X_test, y_test)
print(score)


# 画树--不能识别中文
import graphviz
# feature_name = ['酒精', '苹果酸', '灰', '灰的碱性', '镁', '总酚', '类黄酮', '非黄烷类酚类', '花青素', '颜色强度', '色调', 'od280/od315稀释葡萄酒', '脯氨酸']
# class_name = ["琴酒", '雪莉', '贝尔摩斯'],
feature_name = ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']
class_name = ["01", '02', '03']
"""
# 参数：模型名称、feature_names、class_names、filled、rounded
有feature_names才会按照feature_names命名，否则是名称的索引  class_name同理
filled可以根据class_name进行一个颜色区分
rounded形状变成方形
"""
dot_data = tree.export_graphviz(
    clf,
    feature_names=feature_name,
    class_names=class_name,
    filled=True,
    rounded=True,
)
# 将决策树可视化为图片
import pydotplus
import os
os.environ["PATH"] += os.pathsep + r'C:\Program Files (x86)\graphviz-2.38\bin'
graph = pydotplus.graph_from_dot_data(dot_data)
graph.write_pdf(r"C:\Users\sss\Desktop\wine.pdf")

# 特征重要性判断
print(clf.feature_importances_)
# 为了方便查看
feature_name = ['酒精', '苹果酸', '灰', '灰的碱性', '镁', '总酚', '类黄酮', '非黄烷类酚类', '花青素', '颜色强度', '色调', 'od280/od315稀释葡萄酒', '脯氨酸']
print([*zip(feature_name, clf.feature_importances_)])

# random_state是为了加强预测的准确度，同时保证结果一样
clf = tree.DecisionTreeClassifier(criterion="entropy", random_state=30)
clf = clf.fit(X_train, y_train)
score = clf.score(X_test, y_test)
print(score)

# splitter达成完全随机
clf = tree.DecisionTreeClassifier(
    criterion="entropy",
    random_state=30,
    splitter="random"
)
clf = clf.fit(X_train, y_train)
score = clf.score(X_test, y_test)
print(score)
dot_data = tree.export_graphviz(
    clf,
    feature_names=feature_name,
    class_names=class_name,
    filled=True,
)
graph = pydotplus.graph_from_dot_data(dot_data)
# graph.write_pdf(r"C:\Users\sss\Desktop\wine.pdf")
# 根据实际情况处理，随机性不一定都有效，分越高越好

# 该方法用来检测训练集是否过拟合
score_train = clf.score(X_train, y_train)
print(score_train)


"""
# 剪枝参数--处理上方的过随机性
max_depth：限制树的最大深度      常用！适用高纬度低样本，从3开始
min_samples_leaf：子节点个数 >= min_samples_leaf      搭配max_depth使用，从5开始 
min_samples_split：分支训练样本 >= min_samples_split才分支 
max_features：最大特征个数 <= max_features     少用 
min_impurity_decrease：减小信息增益（父节点的信息熵-子节点的信息熵）
"""
feature_name = ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']
class_name = ["class_0", 'class_1', 'class_2']
clf = tree.DecisionTreeClassifier(
    criterion="entropy",
    random_state=30,
    splitter="random",
    max_depth=3,
    min_samples_leaf=10,
    min_samples_split=10,
)
clf = clf.fit(X_train, y_train)
dot_data = tree.export_graphviz(
    clf,
    feature_names= feature_name,
    class_names= class_name,
    filled=True,
    rounded=True
)
graph = pydotplus.graph_from_dot_data(dot_data)
graph.write_pdf(r"C:\Users\sss\Desktop\wine.pdf")

# 再次查看数据拟合程度
print(clf.score(X_test, y_test))

# 那么对于剪枝，什么时候是最好的呢，就需要循环，测试出最好的效果
# 同时可视化，方便查看
import matplotlib.pyplot as plt

test = []
for i in range(10):
    clf = tree.DecisionTreeClassifier(
        max_depth= i+1,
        criterion= "entropy",
        random_state=30
    )
    clf = clf.fit(X_train, y_train)
    score = clf.score(X_test, y_test)
    test.append(score)
print(test)
plt.plot(range(1, 11), test, color="red", label="max_depth")
plt.legend()
# plt.show()


"""
# 两个重要的接口
apply：返回每个测试样本所在的叶子结点的索引
predict：返回测试样本的分类/回归结果
"""
print(clf.apply(X_test))
print(clf.predict(X_test))
# print(X_test)
print(y_test)
